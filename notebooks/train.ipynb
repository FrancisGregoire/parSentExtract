{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve\n",
    "\n",
    "\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"data/tflogs/\",\n",
    "                       \"Directory to save checkpoints and summaries of the model.\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 2e-4,\n",
    "                      \"Learning rate.\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                      \"Clip gradients to this norm.\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"decision_threshold\", 0.99,\n",
    "                      \"Decision threshold to predict a positive label.\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"keep_prob_input\", 0.8,\n",
    "                      \"Keep probability for dropout applied at embedding layer.\")\n",
    "\n",
    "tf.flags.DEFINE_float(\"keep_prob_output\", 0.7,\n",
    "                      \"Keep probability for dropout applied at prediction layer.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128,\n",
    "                        \"Batch size to use during training.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 300,\n",
    "                        \"Size of each word embedding.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"state_size\", 300,\n",
    "                        \"Size of each hidden state.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"num_layers\", 1,\n",
    "                        \"Number of layers in the model.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 15,\n",
    "                        \"Number of epochs to train the model.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"n_negative\", 7,\n",
    "                        \"Number of negative examples to sample per pair of parallel sentences \"\n",
    "                        \"in training dataset.\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                        \"Number of steps to save a checkpoint.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"source_embeddings_path\", None,\n",
    "                       \"Pretrained embeddings to initialize the source embeddings matrix.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"target_embeddings_path\", None,\n",
    "                       \"Pretrained embeddings to initialize the target embeddings matrix.\")\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"fix_pretrained\", False,\n",
    "                        \"If true fix pretrained embeddings.\")\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"use_lstm\", False,\n",
    "                        \"If true use LSTM cells. Otherwise use GRU cells.\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(sess, data_iterator, threshold, batch_size=128, summary_writer=None):\n",
    "    \"\"\"Evaluate dataset for one epoch.\"\"\"\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    num_iter = int(np.ceil(data_iterator.size / batch_size))\n",
    "    epoch_loss = 0\n",
    "    for step in xrange(num_iter):\n",
    "        source, target, label = data_iterator.next_batch(batch_size)\n",
    "        source_len = utils.sequence_length(source)\n",
    "        target_len = utils.sequence_length(target)\n",
    "        feed_dict = {x_source: source, x_target: target, labels: label,\n",
    "                     source_seq_length: source_len, target_seq_length: target_len,\n",
    "                     decision_threshold: threshold}\n",
    "        loss_value, epoch_accuracy,\\\n",
    "        epoch_precision, epoch_recall = sess.run([mean_loss, accuracy[1],\n",
    "                                                  precision[1], recall[1]],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        epoch_loss += loss_value\n",
    "        if summary_writer and step % FLAGS.steps_per_checkpoint == 0:\n",
    "            summary = sess.run(summaries, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary, global_step=data_iterator.global_step)\n",
    "    epoch_loss /= step\n",
    "    epoch_f1 = utils.f1_score(epoch_precision, epoch_recall)\n",
    "    print(\"  Testing:  Loss = {:.6f}, Accuracy = {:.4f}, \"\n",
    "          \"Precision = {:.4f}, Recall = {:.4f}, F1 = {:.4f}\"\n",
    "          .format(epoch_loss, epoch_accuracy,\n",
    "                  epoch_precision, epoch_recall, epoch_f1))\n",
    "    \n",
    "    \n",
    "def average_pooling(rnn_outputs, seq_length):\n",
    "    sum_rnn_outputs = tf.reduce_sum(tf.concat(rnn_outputs, axis=2), axis=1)\n",
    "    seq_length = tf.expand_dims(tf.cast(seq_length, tf.float32), axis=1)\n",
    "    return tf.divide(sum_rnn_outputs, seq_length)\n",
    "\n",
    "\n",
    "def max_pooling(rnn_outputs):\n",
    "    return tf.reduce_max(tf.concat(rnn_outputs, axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_path = \"data/europarl.en\"\n",
    "target_train_path = \"data/europarl.fr\"\n",
    "source_vocab_path = \"data/vocabulary.en\"\n",
    "target_vocab_path = \"data/vocabulary.fr\"\n",
    "source_vocab_size = 200000\n",
    "target_vocab_size = 200000\n",
    "\n",
    "# Create vocabularies.\n",
    "utils.create_vocabulary(source_vocab_path, source_train_path, source_vocab_size)\n",
    "utils.create_vocabulary(target_vocab_path, target_train_path, target_vocab_size)\n",
    "\n",
    "# Read vocabularies.\n",
    "source_vocab, rev_source_vocab = utils.initialize_vocabulary(source_vocab_path)\n",
    "target_vocab, rev_target_vocab = utils.initialize_vocabulary(target_vocab_path)\n",
    "\n",
    "# Read training data set.\n",
    "parallel_data = utils.read_data(source_train_path, target_train_path, source_vocab, target_vocab)\n",
    "\n",
    "# Read validation data set.\n",
    "source_valid_path = \"data/newstest2012.en\"\n",
    "target_valid_path = \"data/newstest2012.fr\"\n",
    "n_eval = 1000\n",
    "valid_data = utils.read_data(source_valid_path, target_valid_path, source_vocab, target_vocab)\n",
    "valid_data = valid_data[:n_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_data[-1])\n",
    "print()\n",
    "print(' '.join(rev_source_vocab[i] for i in parallel_data[-1][0]))\n",
    "print()\n",
    "print(' '.join(rev_target_vocab[i] for i in parallel_data[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_data[-1])\n",
    "print(' '.join(rev_source_vocab[i] for i in valid_data[-1][0]))\n",
    "print()\n",
    "print(' '.join(rev_target_vocab[i] for i in valid_data[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.reset_graph()\n",
    "\n",
    "# Params\n",
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "learning_rate = FLAGS.learning_rate\n",
    "embedding_size = FLAGS.embedding_size\n",
    "state_size = FLAGS.state_size\n",
    "batch_size = FLAGS.batch_size\n",
    "num_layers = FLAGS.num_layers\n",
    "max_gradient_norm = FLAGS.max_gradient_norm\n",
    "use_lstm = FLAGS.use_lstm\n",
    "fix_pretrained = FLAGS.fix_pretrained\n",
    "source_embeddings_path = FLAGS.source_embeddings_path\n",
    "target_embeddings_path = FLAGS.target_embeddings_path\n",
    "\n",
    "use_average_pooling = False\n",
    "use_max_pooling = False\n",
    "pred_hidden_size = 128\n",
    "\n",
    "# Placeholders.\n",
    "x_source = tf.placeholder(tf.int32, [None, None], name=\"x_source\")\n",
    "source_seq_length = tf.placeholder(tf.int32, [None], name=\"source_seq_length\")\n",
    "\n",
    "x_target = tf.placeholder(tf.int32, [None, None], name=\"x_target\")\n",
    "target_seq_length = tf.placeholder(tf.int32, [None], name=\"target_seq_length\")\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [None], name=\"labels\")\n",
    "\n",
    "input_dropout = tf.placeholder_with_default(1.0, [], name=\"input_dropout\")\n",
    "output_dropout = tf.placeholder_with_default(1.0, [], name=\"output_dropout\")\n",
    "\n",
    "decision_threshold = tf.placeholder_with_default(0.5, [], name=\"decision_threshold\")\n",
    "\n",
    "# Embedding layer.\n",
    "with tf.variable_scope(\"embeddings\"):\n",
    "    if source_embeddings_path is not None and target_embeddings_path is not None:\n",
    "        source_pretrained_embeddings,\\\n",
    "        target_pretrained_embeddings = get_pretrained_embeddings(\n",
    "            source_embeddings_path,\n",
    "            target_embeddings_path,\n",
    "            source_vocab,\n",
    "            target_vocab)\n",
    "        assert source_pretrained_embeddings.shape[1] == target_pretrained_embeddings.shape[1]\n",
    "        embedding_size = source_pretrained_embeddings.shape[1]\n",
    "        if fix_pretrained:\n",
    "            source_embeddings = tf.get_variable(\n",
    "                \"source_embeddings_matrix\",\n",
    "                [source_vocab_size, embedding_size],\n",
    "                initializer=tf.constant_initializer(source_pretrained_embeddings),\n",
    "                trainable=False)\n",
    "            target_embeddings = tf.get_variable(\n",
    "                \"target_embeddings_matrix\",\n",
    "                [target_vocab_size, embedding_size],\n",
    "                initializer=tf.constant_initializer(target_pretrained_embeddings),\n",
    "                trainable=False)\n",
    "        else:\n",
    "            source_embeddings = tf.get_variable(\n",
    "                \"source_embeddings_matrix\",\n",
    "                [source_vocab_size, embedding_size],\n",
    "                initializer=tf.constant_initializer(source_pretrained_embeddings))\n",
    "            target_embeddings = tf.get_variable(\n",
    "                \"target_embeddings_matrix\",\n",
    "                [target_vocab_size, embedding_size],\n",
    "                initializer=tf.constant_initializer(target_pretrained_embeddings))\n",
    "    else:\n",
    "        source_embeddings = tf.get_variable(\"source_embeddings_matrix\",\n",
    "                                            [source_vocab_size, embedding_size])\n",
    "        target_embeddings = tf.get_variable(\"target_embeddings_matrix\",\n",
    "                                            [target_vocab_size, embedding_size])\n",
    "    source_rnn_inputs = tf.nn.embedding_lookup(source_embeddings, x_source)\n",
    "    target_rnn_inputs = tf.nn.embedding_lookup(target_embeddings, x_target)\n",
    "    source_rnn_inputs = tf.nn.dropout(source_rnn_inputs, keep_prob=input_dropout,\n",
    "                                      name=\"source_seq_embeddings\")\n",
    "    target_rnn_inputs = tf.nn.dropout(target_rnn_inputs, keep_prob=input_dropout,\n",
    "                                      name=\"target_seq_embeddings\")\n",
    "\n",
    "# Siamese BiRNN.\n",
    "with tf.variable_scope(\"siamese_birnn\") as scope:\n",
    "    if use_lstm:\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(state_size, use_peepholes=True)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(state_size, use_peepholes=True)\n",
    "    else:\n",
    "        cell_fw = tf.contrib.rnn.GRUCell(state_size)\n",
    "        cell_bw = tf.contrib.rnn.GRUCell(state_size)\n",
    "\n",
    "    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=output_dropout)\n",
    "    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=output_dropout)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw for _ in xrange(num_layers)])\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw for _ in xrange(num_layers)])\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        source_rnn_outputs, source_final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n",
    "                                                                                 cell_bw=cell_bw,\n",
    "                                                                                 inputs=source_rnn_inputs,\n",
    "                                                                                 sequence_length=source_seq_length,\n",
    "                                                                                 dtype=tf.float32)\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        target_rnn_outputs, target_final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n",
    "                                                                                 cell_bw=cell_bw,\n",
    "                                                                                 inputs=target_rnn_inputs,\n",
    "                                                                                 sequence_length=target_seq_length,\n",
    "                                                                                 dtype=tf.float32)\n",
    "    state_size *= 2\n",
    "    # Mean pooling only works for 1 layer bidirectional GRU.\n",
    "    if use_average_pooling:\n",
    "        source_final_state = average_pooling(source_rnn_outputs, source_seq_length)\n",
    "        target_final_state = average_pooling(target_rnn_outputs, target_seq_length)\n",
    "    elif use_max_pooling:\n",
    "        source_final_state = max_pooling(source_rnn_outputs)\n",
    "        target_final_state = max_pooling(target_rnn_outputs)\n",
    "    else:\n",
    "        source_final_state_fw, source_final_state_bw = source_final_state\n",
    "        target_final_state_fw, target_final_state_bw = target_final_state\n",
    "        if num_layers > 1:\n",
    "            source_final_state_fw = source_final_state_fw[-1]\n",
    "            source_final_state_bw = source_final_state_bw[-1]\n",
    "            target_final_state_fw = target_final_state_fw[-1]\n",
    "            target_final_state_bw = target_final_state_bw[-1]\n",
    "        if use_lstm:\n",
    "            source_final_state_fw = source_final_state_fw.h\n",
    "            source_final_state_bw = source_final_state_bw.h\n",
    "            target_final_state_fw = target_final_state_fw.h\n",
    "            target_final_state_bw = target_final_state_bw.h\n",
    "        source_final_state = tf.concat([source_final_state_fw, source_final_state_bw], axis=1)\n",
    "        target_final_state = tf.concat([target_final_state_fw, target_final_state_bw], axis=1)\n",
    "\n",
    "# Prediction layer.\n",
    "with tf.variable_scope(\"prediction\"):\n",
    "    h_multiply = tf.multiply(source_final_state, target_final_state)\n",
    "    h_abs_diff = tf.abs(tf.subtract(source_final_state, target_final_state))\n",
    "\n",
    "    W_1 = tf.get_variable(\"W_1\", [state_size, pred_hidden_size])\n",
    "    W_2 = tf.get_variable(\"W_2\", [state_size, pred_hidden_size])\n",
    "    b_1 = tf.get_variable(\"b_1\", [pred_hidden_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    h_semantic = tf.tanh(tf.matmul(h_multiply, W_1) + tf.matmul(h_abs_diff, W_2) + b_1)\n",
    "\n",
    "    W_3 = tf.get_variable(\"W_3\", [pred_hidden_size, 1]) # try initializer\n",
    "    b_2 = tf.get_variable(\"b_2\", [1], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    logits = tf.matmul(h_semantic, W_3) + b_2\n",
    "    logits = tf.squeeze(logits, name = \"logits\")\n",
    "\n",
    "# Sigmoid values and evaluation metrics.\n",
    "with tf.name_scope(\"prediction_evaluation\"):\n",
    "    probs = tf.sigmoid(logits, name=\"probs\")\n",
    "    predicted_class = tf.cast(tf.greater(probs, decision_threshold), tf.float32, name=\"predicted_class\")\n",
    "    accuracy = tf.metrics.accuracy(labels, predicted_class, name=\"accuracy\")\n",
    "    precision = tf.metrics.precision(labels, predicted_class, name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels, predicted_class, name=\"recall\")\n",
    "\n",
    "# Loss.\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels,\n",
    "                                                     name=\"cross_entropy_per_sequence\")\n",
    "    mean_loss = tf.reduce_mean(losses, name=\"cross_entropy_loss\")\n",
    "\n",
    "# Optimization.\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    gradients = tf.gradients(mean_loss, trainable_variables, name=\"gradients\")\n",
    "    clipped_gradients, global_norm = tf.clip_by_global_norm(gradients, max_gradient_norm,\n",
    "                                                            name=\"clipped_gradients\")\n",
    "    train_op = optimizer.apply_gradients(zip(clipped_gradients, trainable_variables), global_step=global_step)\n",
    "\n",
    "# Summaries.\n",
    "tf.summary.scalar(\"loss\", mean_loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy[0])\n",
    "tf.summary.scalar(\"precision\", precision[0])\n",
    "tf.summary.scalar(\"recall\", recall[0])\n",
    "tf.summary.scalar(\"logits\" + \"/sparsity\", tf.nn.zero_fraction(logits))\n",
    "tf.summary.histogram(\"logits\" + \"/activations\", logits)\n",
    "tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Add histogram for trainable variables.\n",
    "for var in trainable_variables:\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "# Add histogram for gradients.\n",
    "for grad, var in zip(clipped_gradients, trainable_variables):\n",
    "    if grad is not None:\n",
    "        tf.summary.histogram(var.op.name + \"/gradients\", grad)\n",
    "tf.summary.scalar(\"global_norm\", global_norm)\n",
    "        \n",
    "summaries = tf.summary.merge_all()\n",
    "tf.add_to_collection(tf.GraphKeys.SUMMARY_OP, summaries)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "train_iterator = utils.TrainingIteratorRandom(parallel_data, FLAGS.n_negative)\n",
    "valid_iterator = utils.EvalIterator(valid_data)\n",
    "\n",
    "train_summary_writer = tf.summary.FileWriter(os.path.join(FLAGS.checkpoint_dir, \"train\"), sess.graph)\n",
    "valid_summary_writer = tf.summary.FileWriter(os.path.join(FLAGS.checkpoint_dir, \"valid\"), sess.graph)\n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_completed = 0\n",
    "batch_completed = 0\n",
    "\n",
    "num_iter = int(np.ceil(train_iterator.size / FLAGS.batch_size * FLAGS.num_epochs))\n",
    "start_time = time.time()\n",
    "print(\"Training model for {} sentence pairs & validating on {} sentence pairs\".\n",
    "      format(train_iterator.size, valid_iterator.size))\n",
    "\n",
    "for step in xrange(num_iter):\n",
    "    source, target, label = train_iterator.next_batch(FLAGS.batch_size)\n",
    "    source_len = utils.sequence_length(source)\n",
    "    target_len = utils.sequence_length(target)\n",
    "    feed_dict = {x_source: source, x_target: target, labels: label,\n",
    "                 source_seq_length: source_len, target_seq_length: target_len,\n",
    "                 input_dropout: FLAGS.keep_prob_input, output_dropout: FLAGS.keep_prob_output,\n",
    "                 decision_threshold: FLAGS.decision_threshold}\n",
    "\n",
    "    _, loss_value, epoch_accuracy,\\\n",
    "    epoch_precision, epoch_recall = sess.run([train_op, mean_loss, accuracy[1],\n",
    "                                              precision[1], recall[1]], feed_dict=feed_dict)\n",
    "    epoch_loss += loss_value\n",
    "    batch_completed += 1\n",
    "    # Write the model's training summaries.\n",
    "    if step % FLAGS.steps_per_checkpoint == 0:\n",
    "        summary = sess.run(summaries, feed_dict=feed_dict)\n",
    "        train_summary_writer.add_summary(summary, global_step=step)\n",
    "    # End of current epoch.\n",
    "    if train_iterator.epoch_completed > epoch_completed:\n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_loss /= batch_completed\n",
    "        epoch_f1 = utils.f1_score(epoch_precision, epoch_recall)\n",
    "        epoch_completed += 1\n",
    "        print(\"Epoch {} in {:.0f} sec\\n\"\n",
    "              \"  Training: Loss = {:.6f}, Accuracy = {:.4f}, \"\n",
    "              \"Precision = {:.4f}, Recall = {:.4f}, F1 = {:.4f}\"\n",
    "              .format(epoch_completed, epoch_time, epoch_loss, epoch_accuracy,\n",
    "                      epoch_precision, epoch_recall, epoch_f1))\n",
    "        # Save a checkpoint.\n",
    "        checkpoint_path = os.path.join(FLAGS.checkpoint_dir, \"model.ckpt\")\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n",
    "        # Evaluate model on the validation set.\n",
    "        eval_epoch(sess, valid_iterator, FLAGS.decision_threshold, 1000, valid_summary_writer)\n",
    "        # Initialize local variables for new epoch.\n",
    "        batch_completed = 0\n",
    "        epoch_loss = 0\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        start_time = time.time()\n",
    "\n",
    "print(\"Training done with {} steps.\".format(sess.run(global_step, feed_dict=feed_dict)))\n",
    "train_summary_writer.close()\n",
    "valid_summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sess, data_iterator, probs_op, predicted_class_op, threshold, placeholders):\n",
    "    \"\"\"Get probability and predicted class of the examples in a data set.\"\"\"\n",
    "    x_source, source_seq_length,\\\n",
    "    x_target, target_seq_length,\\\n",
    "    labels, decision_threshold = placeholders\n",
    "\n",
    "    num_iter = int(np.ceil(data_iterator.size / FLAGS.batch_size))\n",
    "    probs = []\n",
    "    predicted_class = []\n",
    "    for step in xrange(num_iter):\n",
    "        source, target, label = data_iterator.next_batch(FLAGS.batch_size)\n",
    "        source_len = utils.sequence_length(source)\n",
    "        target_len = utils.sequence_length(target)\n",
    "\n",
    "        feed_dict = {x_source: source, x_target: target, labels: label,\n",
    "                     source_seq_length: source_len, target_seq_length: target_len,\n",
    "                     decision_threshold: threshold}\n",
    "        batch_probs, batch_predicted_class = sess.run([probs_op, predicted_class_op], feed_dict=feed_dict)\n",
    "        probs.extend(batch_probs.tolist())\n",
    "        predicted_class.extend(batch_predicted_class.tolist())\n",
    "    probs = np.array(probs[:data_iterator.size])\n",
    "    predicted_class = np.array(predicted_class[:data_iterator.size], dtype=np.int)\n",
    "    return probs, predicted_class\n",
    "\n",
    "\n",
    "def evaluate(sess, data_iterator, probs_op, predicted_class_op, threshold, placeholders, save, plot):\n",
    "    probs, predicted_class = inference(sess, data_iterator, probs_op, predicted_class_op, threshold, placeholders)\n",
    "    true_class = data_iterator.epoch_data[:, 2].astype(int)\n",
    "\n",
    "    # Evaluation at given decision threshold.\n",
    "    precision_value = precision_score(true_class, predicted_class, pos_label=1)\n",
    "    recall_value = recall_score(true_class, predicted_class, pos_label=1)\n",
    "    f1_value = f1_score(true_class, predicted_class, pos_label=1)\n",
    "\n",
    "    print(\"Evaluation metrics at decision threshold = {:.4f}\\n\"\n",
    "          \"Precision = {:.2f}, Recall = {:.2f}, F1 = {:.2f}\\n\"\n",
    "          \"-------------------------------------------------\"\n",
    "          .format(threshold, 100*precision_value, 100*recall_value, 100*f1_value))\n",
    "\n",
    "    # Evaluation at best F1 value.\n",
    "    precision_seq, recall_seq, threshold_seq = precision_recall_curve(true_class, probs, pos_label=1)\n",
    "    f1_seq = utils.f1_score(precision_seq, recall_seq)\n",
    "    index = np.argmax(f1_seq)\n",
    "    precision_best = precision_seq[index]\n",
    "    recall_best = recall_seq[index]\n",
    "    f1_best = f1_seq[index]\n",
    "    threshold_best = threshold_seq[index]\n",
    "\n",
    "    print(\"Best scores if decision threshold = {:.4f}\\n\"\n",
    "          \"Precision = {:.2f}, Recall = {:.2f}, F1 = {:.2f}\"\n",
    "          .format(threshold_best, 100*precision_best, 100*recall_best, 100*f1_best))\n",
    "\n",
    "    if save:\n",
    "        np.savez(save, precision_value=precision_value, recall_value=recall_value,\n",
    "                 f1_value=f1_value, threshold_value=threshold,\n",
    "                 precision_best=precision_best, recall_best=recall_best,\n",
    "                 f1_best=f1_best, threshold_best=threshold_best,\n",
    "                 precision_seq=precision_seq, recall_seq=recall_seq,\n",
    "                 f1_seq=f1_seq, threshold_seq=threshold_seq)\n",
    "    if plot:\n",
    "        utils.plot_precision_recall_curve(precision_seq, recall_seq, \"\")\n",
    "        utils.plot_f1_threshold_curve(threshold_seq, f1_seq, \"\")\n",
    "\n",
    "        \n",
    "# Run evaluation.\n",
    "source_test_path = \"data/newstest2013.en\"\n",
    "target_test_path = \"data/newstest2013.fr\"\n",
    "n_eval = 1000\n",
    "test_data = utils.read_data(source_test_path, target_test_path, source_vocab, target_vocab)\n",
    "test_data = test_data[:n_eval]\n",
    "test_iterator = utils.EvalIterator(test_data)\n",
    "placeholders = [x_source, source_seq_length, x_target, target_seq_length, labels, decision_threshold]\n",
    "evaluate(sess, test_iterator, probs, predicted_class, FLAGS.decision_threshold,\n",
    "         placeholders, None, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
