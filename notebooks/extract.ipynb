{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "\n",
    "from itertools import product\n",
    "from six.moves import xrange\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "def read_articles(filename):\n",
    "    \"\"\"Read the articles from a given file. Yields (index, article) tuples.\"\"\"\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as input_file:\n",
    "        txt, index = None, None\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            match = re.match(r'<article( id=\"(.*?)\")?( \\w+=.*?)*>', line)\n",
    "            if match:\n",
    "                txt = \"\"\n",
    "                index = match.group(2)\n",
    "            elif line == \"</article>\" and txt is not None:\n",
    "                yield (index, txt)\n",
    "                txt = None\n",
    "            elif txt is not None:\n",
    "                txt += line + \"\\n\"\n",
    "\n",
    "\n",
    "def inference(sess, data_iterator, probs_op, predicted_class_op, placeholders, batch_size, threshold):\n",
    "    \"\"\"Get probability and predicted class of the examples in a data set.\"\"\"\n",
    "    x_source, source_seq_length,\\\n",
    "    x_target, target_seq_length,\\\n",
    "    labels, decision_threshold = placeholders\n",
    "\n",
    "    num_iter = int(np.ceil(data_iterator.size / batch_size))\n",
    "    probs = []\n",
    "    predicted_class = []\n",
    "    for step in xrange(num_iter):\n",
    "        source, target, label = data_iterator.next_batch(batch_size)\n",
    "        source_len = utils.sequence_length(source)\n",
    "        target_len = utils.sequence_length(target)\n",
    "\n",
    "        feed_dict = {x_source: source, x_target: target, labels: label,\n",
    "                     source_seq_length: source_len, target_seq_length: target_len,\n",
    "                     decision_threshold: threshold}\n",
    "        batch_probs, batch_predicted_class = sess.run([probs_op, predicted_class_op], feed_dict=feed_dict)\n",
    "        probs.extend(batch_probs.tolist())\n",
    "        predicted_class.extend(batch_predicted_class.tolist())\n",
    "    probs = np.array(probs[:data_iterator.size])\n",
    "    predicted_class = np.array(predicted_class[:data_iterator.size], dtype=np.int)\n",
    "    return probs, predicted_class\n",
    "\n",
    "\n",
    "def extract_pairs(sess, alignment_model, source_sentences, target_sentences, \n",
    "                  source_sentences_ids, target_sentences_ids,\n",
    "                  probs_op, predicted_class_op, placeholders, \n",
    "                  batch_size, threshold, greedy=False):\n",
    "    \"\"\"Extract sentence pairs from articles pairs.\n",
    "       Returns a list of (source sentence, target sentence, probability score) tuples.\n",
    "    \"\"\"\n",
    "    data = [(source_sentences_ids[i], target_sentences_ids[j])\n",
    "            for i, j in product(range(len(source_sentences)), range(len(target_sentences)))]\n",
    "    pairs = [(i, j) for i, j in product(range(len(source_sentences)), range(len(target_sentences)))]\n",
    "    \n",
    "    data_iterator = EvalIterator(np.array(data, dtype=object))\n",
    "    \n",
    "    y_score, y_label = inference(sess, data_iterator, probs_op, predicted_class_op,\n",
    "                                 placeholders, batch_size, threshold)\n",
    "\n",
    "    sentence_pairs = []\n",
    "    if greedy:\n",
    "        alignments = [(s, k) for k, s in enumerate(y_score)]\n",
    "        alignments.sort(reverse=True)\n",
    "        seen_src = set()\n",
    "        seen_trg = set()\n",
    "        for s, k in alignments:\n",
    "            i, j = pairs[k]\n",
    "            if s < threshold or i in seen_src or j in seen_trg:\n",
    "                continue\n",
    "            if greedy:\n",
    "                seen_src.add(i)\n",
    "                seen_trg.add(j)\n",
    "            sentence_pairs.append((source_sentences[i], target_sentences[j], s))\n",
    "    else:\n",
    "        idx = np.where(y_label == 1)[0]\n",
    "        if len(idx) > 0:\n",
    "            for k in idx:\n",
    "                i, j = pairs[k]\n",
    "                sentence_pairs.append((source_sentences[i], target_sentences[j], y_score[k]))\n",
    "\n",
    "    return sentence_pairs\n",
    "\n",
    "\n",
    "class EvalIterator(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.global_step = 0\n",
    "        self.epoch_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    def _sequence_length(self, data):\n",
    "        length = np.zeros((len(data), 2), dtype=np.int32)\n",
    "        for i, data_i in enumerate(data):\n",
    "            source, target = data_i\n",
    "            length[i] = (len(source), len(target))\n",
    "        return length\n",
    "\n",
    "    def _pad_batch(self, data):\n",
    "        batch_size = len(data)\n",
    "        batch_sequence_length = self._sequence_length(data)\n",
    "        max_sequence_length = np.max(batch_sequence_length, axis=0)\n",
    "        source, target = np.hsplit(data, 2)\n",
    "        pad_source = np.zeros((batch_size, max_sequence_length[0]), dtype=np.int32)\n",
    "        pad_target = np.zeros((batch_size, max_sequence_length[1]), dtype=np.int32)\n",
    "        for i in xrange(batch_size):\n",
    "            pad_source[i, :batch_sequence_length[i, 0]] = source[i, 0]\n",
    "            pad_target[i, :batch_sequence_length[i, 1]] = target[i, 0]\n",
    "        return pad_source, pad_target, np.ones(batch_size)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        self.global_step += 1\n",
    "        start = self._index_in_epoch\n",
    "        if start + batch_size > self.size:\n",
    "            self.epoch_completed += 1\n",
    "            size_not_observed = self.size - start\n",
    "            data_not_observed = self.data[start:self.size]\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - size_not_observed\n",
    "            end = self._index_in_epoch\n",
    "            batch_data = np.concatenate((data_not_observed, self.data[start:end]), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            batch_data = self.data[start:end]\n",
    "        return self._pad_batch(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and restore trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore saved TensorFlow model.\n",
    "sess = tf.Session()\n",
    "checkpoint_dir = \"data/tflogs\"\n",
    "saver = tf.train.import_meta_graph(\"data/tflogs/model.ckpt-410156.meta\")\n",
    "saver.restore(sess, \"data/tflogs/model.ckpt-410156\")\n",
    "        \n",
    "# Recover placeholders and ops for extraction.\n",
    "x_source = sess.graph.get_tensor_by_name(\"x_source:0\")\n",
    "source_seq_length = sess.graph.get_tensor_by_name(\"source_seq_length:0\")\n",
    "\n",
    "x_target = sess.graph.get_tensor_by_name(\"x_target:0\")\n",
    "target_seq_length = sess.graph.get_tensor_by_name(\"target_seq_length:0\")\n",
    "\n",
    "labels = sess.graph.get_tensor_by_name(\"labels:0\")\n",
    "\n",
    "decision_threshold = sess.graph.get_tensor_by_name(\"decision_threshold:0\")\n",
    "\n",
    "placeholders = [x_source, source_seq_length, x_target, target_seq_length, labels, decision_threshold]\n",
    "\n",
    "probs = sess.graph.get_tensor_by_name(\"prediction_evaluation/probs:0\")\n",
    "predicted_class = sess.graph.get_tensor_by_name(\"prediction_evaluation/predicted_class:0\")\n",
    "\n",
    "placeholders = [x_source, source_seq_length, x_target, target_seq_length, labels, decision_threshold]\n",
    "\n",
    "# Read vocabularies.\n",
    "source_vocab_path = \"data/vocabulary.en\"\n",
    "target_vocab_path = \"data/vocabulary.fr\"\n",
    "source_vocab, rev_source_vocab = utils.initialize_vocabulary(source_vocab_path)\n",
    "target_vocab, rev_target_vocab = utils.initialize_vocabulary(target_vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence pairs from article pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_filename = \"data/wikipedia.en\"\n",
    "trg_filename = \"data/wikipedia.fr\"\n",
    "\n",
    "src_articles = read_articles(src_filename)\n",
    "trg_articles = read_articles(trg_filename)\n",
    "\n",
    "src_output = \"data/wikipedia_extracted.en\"\n",
    "trg_output = \"data/wikipedia_extracted.fr\"\n",
    "s_output = \"data/wikipedia_extracted.s\"\n",
    "\n",
    "threshold = 0.99\n",
    "batch_size = 2000\n",
    "greedy = False\n",
    "n_articles = 919000\n",
    "\n",
    "with open(src_output, mode=\"w\", encoding=\"utf-8\") as src_output_file,\\\n",
    "     open(trg_output, mode=\"w\", encoding=\"utf-8\") as trg_output_file,\\\n",
    "     open(s_output, mode=\"w\", encoding=\"utf-8\") as s_output_file:\n",
    "\n",
    "    for (index, src_txt), (_, trg_txt) in zip(src_articles, trg_articles):\n",
    "        \n",
    "        source_sentences = src_txt.split(\"\\n\")\n",
    "        target_sentences = trg_txt.split(\"\\n\")\n",
    "        \n",
    "        source_sentences_ids = [utils.sentence_to_token_ids(sent, source_vocab, 100) for sent in source_sentences]\n",
    "        target_sentences_ids = [utils.sentence_to_token_ids(sent, target_vocab, 100) for sent in target_sentences]\n",
    "        \n",
    "        pairs = extract_pairs(sess, alignment_model, source_sentences, target_sentences,\n",
    "                              source_sentences_ids, target_sentences_ids,\n",
    "                              probs, predicted_class, placeholders, \n",
    "                              batch_size, threshold, filtering, greedy)\n",
    "        \n",
    "        if int(index) % 50000 == 0:\n",
    "            print(\"{:.2f}% done.\".format(100 * int(index) / n_articles))\n",
    "            \n",
    "        if not pairs:\n",
    "            continue\n",
    "\n",
    "        for src_line, trg_line, s in pairs:\n",
    "            src_output_file.write(src_line + \"\\n\")\n",
    "            trg_output_file.write(trg_line + \"\\n\")\n",
    "            s_output_file.write(str(s).encode(\"utf-8\").decode(\"utf-8\") + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
